---
title: 'Ecce Signum: Illustration'
output:
  html_document:
    df_print: paged
---

```{r, setup, include=FALSE}
#knitr::opts_knit$set(root.dir = 'C:/Users/neide/Documents/GitHub/sigex')
```
 
# Business Formation Statistics

We study a weekly univariate series: the business applications component of the
Business Formation Statistics, or **bfs** for short. (Weekly data as of week 27
of 2020, downloaded on July 14, 2020 (11:27 AM), U.S. Census Bureau, obtained 
from https://www.census.gov/econ/bfs/index.html.) We consider the non-seasonally 
adjusted data at the national level. This component is the main business 
applications series, which captures the weekly flow of IRS applications for 
Employer Identification Numbers, mainly for business purposes.
    
## Loading Packages 

This code installs and loads the packages, and loads the data into a variable 
named **bfs**.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
library(devtools)
library(Rcpp)
devtools::install_github("tuckermcelroy/sigex")
library(sigex)
```
    
## Enter Metadata

- We perform a univariate analysis by fitting a SARMA model.
- First enter metadata, including starting date and frequency.
   	 
```{r}
begin <- c(2006,1)
end <- c(2020,27)
period <- 52
```
   	 
- Next enter names of the series.
- The last argument of **sigex.load** generates a time series plot.

```{r}
dataALL.ts <- sigex.load(bfs[,3:6],begin,period,
                         c("bfs-ba","bfs-hba","bfs-wba","bfs-cba"),FALSE)
```
 
## Select Spans and Transforms

- We have the choice of either log or no log.
- The *aggregate* option will sum across the indicated series, in case we want 
to analyze an aggregate. Here we set it to FALSE.
- We can also select a subcomponent of series with *subseries*.
- The *range* argument of **sigex.prep** will select a subset of dates for the 
time series to be analyzed.
- We select one of four national series corresponding to business applications, 
and choose a log transformation.   
 
```{r}
transform <- "log"
aggregate <- FALSE
subseries <- 1
range <- NULL
data.ts <- sigex.prep(dataALL.ts,transform,aggregate,subseries,range,TRUE)
```

## Spectral Exploratory Analysis

- In order to get an idea about model specification, we can examine spectral 
estimates, using **sigex.specar**.
- We can look at raw data or differenced data (growth rates).

```{r}
# raw data
par(mfrow=c(1,1))
for(i in 1:length(subseries)) { sigex.specar(data.ts,FALSE,i,period) }
```

```{r}
# growth rates
par(mfrow=c(1,1))
for(i in 1:length(subseries)) { sigex.specar(data.ts,TRUE,i,period) }
```

##  Model Declaration

- Now we want to define our SARMA model.
- We begin by defining dimension $N$ and sample size $T$.

```{r}
N <- dim(data.ts)[2]
T <- dim(data.ts)[1]
```

- To specify the holiday regressors, we need to know the calendar date (in 
month-day-year format) for the beginning and end of the series, i.e., the first 
day of the first week and the last day of the last week.  
- The function **weekly2date** provides this.
- Weekly data typically corresponds to measurements over a seven-day period 
beginning with Sunday, in which *first.day* is set equal to $1$.

```{r}
first.day <- 1
all.date <- weekly2date(first.day,begin,T)
start.date <- all.date[[1]]
end.date <- all.date[[2]]
```

- Next, we load the holiday regressors.
- One supplies an ASCI file  whose every row contains a calendar date for the 
relevant holiday, covering a broad range of years.
- Regressors can simply be created as a daily time series by **gethol**, and 
then later converted to a weekly regressor.  
- We use a window of $7$ days before and $0$ days after, so that the holiday
effects are presumed to constitute eight days.
- In the calls to **gethol**, both *start.date* and *end.date* are required.
   
```{r}
easter.dates <- read.table(paste(root.dir,"/data/easter500.txt",sep=""))
easter.reg <- gethol(easter.dates,7,0,start.date,end.date)

nyd.dates <- read.table(paste(root.dir,"/data/newyear500.txt",sep=""))
nyd.reg <- gethol(nyd.dates,7,0,start.date,end.date)

mlk.dates <- read.table(paste(root.dir,"/data/mlk500.txt",sep=""))
mlk.reg <- gethol(mlk.dates,7,0,start.date,end.date)

gw.dates <- read.table(paste(root.dir,"/data/gw500.txt",sep=""))
gw.reg <- gethol(gw.dates,7,0,start.date,end.date)

mem.dates <- read.table(paste(root.dir,"/data/mem500.txt",sep=""))
mem.reg <- gethol(mem.dates,7,0,start.date,end.date)

ind.dates <- read.table(paste(root.dir,"/data/ind500.txt",sep=""))
ind.reg <- gethol(ind.dates,7,0,start.date,end.date)

labor.dates <- read.table(paste(root.dir,"/data/labor500.txt",sep=""))
labor.reg <- gethol(labor.dates,7,0,start.date,end.date)

col.dates <- read.table(paste(root.dir,"/data/columbus500.txt",sep=""))
col.reg <- gethol(col.dates,7,0,start.date,end.date)

vet.dates <- read.table(paste(root.dir,"/data/vet500.txt",sep=""))
vet.reg <- gethol(vet.dates,7,0,start.date,end.date)

tg.dates <- read.table(paste(root.dir,"/data/thanksgiving500.txt",sep=""))
tg.reg <- gethol(tg.dates,7,0,start.date,end.date)

xmas.dates <- read.table(paste(root.dir,"/data/xmas500.txt",sep=""))
xmas.reg <- gethol(xmas.dates,7,0,start.date,end.date)

black.dates <- read.table(paste(root.dir,"/data/black400.txt",sep=""))
black.reg <- gethol(black.dates,7,0,start.date,end.date)
```

- The daily regressors are converted to weekly flow regressors by first 
embedding with *sigex.daily2weekly*, followed by averaging over each week.

```{r}
easter.reg <- sigex.daily2weekly(easter.reg,first.day,start.date)
easter.reg <- rowSums(easter.reg)/7

nyd.reg <- sigex.daily2weekly(nyd.reg,first.day,start.date)
nyd.reg <- rowSums(nyd.reg)/7

mlk.reg <- sigex.daily2weekly(mlk.reg,first.day,start.date)
mlk.reg <- rowSums(mlk.reg)/7

gw.reg <- sigex.daily2weekly(gw.reg,first.day,start.date)
gw.reg <- rowSums(gw.reg)/7

mem.reg <- sigex.daily2weekly(mem.reg,first.day,start.date)
mem.reg <- rowSums(mem.reg)/7

ind.reg <- sigex.daily2weekly(ind.reg,first.day,start.date)
ind.reg <- rowSums(ind.reg)/7

labor.reg <- sigex.daily2weekly(labor.reg,first.day,start.date)
labor.reg <- rowSums(labor.reg)/7

col.reg <- sigex.daily2weekly(col.reg,first.day,start.date)
col.reg <- rowSums(col.reg)/7

vet.reg <- sigex.daily2weekly(vet.reg,first.day,start.date)
vet.reg <- rowSums(vet.reg)/7

tg.reg <- sigex.daily2weekly(tg.reg,first.day,start.date)
tg.reg <- rowSums(tg.reg)/7

xmas.reg <- sigex.daily2weekly(xmas.reg,first.day,start.date)
xmas.reg <- rowSums(xmas.reg)/7

black.reg <- sigex.daily2weekly(black.reg,first.day,start.date)
black.reg <- rowSums(black.reg)/7
```

## Model Construction and Fitting

### Initial Model: no holidays

- We fit a baseline model that involves no fixed effects
- The model has a SARMA specification with order one for the AR, MA, seasonal 
AR, and seasonal MA polynomials.

```{r}
mdl <- NULL
mdl <- sigex.add(mdl,seq(1,N),"sarma",c(1,1,1,1,52),NULL,"process",1)
mdl <- sigex.meaninit(mdl,data.ts,0)
```

- Maximum Likelihood Estimation is done using BFGS. We use the divergence, which
is $-2$ times the log Gaussian likelihood, with constants removed.
- With the debug option set to TRUE, the values of the divergence are printed to 
the console.

```{r}
constraint <- NULL
par.mle <- sigex.default(mdl,data.ts,constraint)
psi.mle <- sigex.par2psi(par.mle,mdl)
fit.mle <- sigex.mlefit(data.ts,par.mle,constraint,mdl,"bfgs",debug=FALSE)
psi.mle <- sigex.eta2psi(fit.mle[[1]]$par,constraint)
hess <- fit.mle[[1]]$hessian
par.mle <- fit.mle[[2]]
```

- This yields a divergence of `r sigex.lik(psi.mle,mdl,data.ts)`.
 
### Improved Model: add holidays

- The next model incorporates the ten federal holidays along with Easter and 
Black Friday.
- Note that Independence Day, Veteran's Day, and Christmas are not included by 
the call to *sigex.reg* (which checks for regressors that are numerically zero 
after differencing).

```{r}
mdl <- NULL
mdl <- sigex.add(mdl,seq(1,N),"sarma",c(1,1,1,1,52),NULL,"process",1)
mdl <- sigex.meaninit(mdl,data.ts,0)
mdl <- sigex.reg(mdl,1,ts(as.matrix(easter.reg),start=start(easter.reg),
                          frequency=period,names="Easter"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(nyd.reg),start=start(nyd.reg),
                          frequency=period,names="NewYearDay"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(mlk.reg),start=start(mlk.reg),
                          frequency=period,names="MLK"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(gw.reg),start=start(gw.reg),
                          frequency=period,names="GeorgeWashington"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(mem.reg),start=start(mem.reg),
                          frequency=period,names="MemorialDay"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(ind.reg),start=start(ind.reg),
                          frequency=period,names="IndependenceDay"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(labor.reg),start=start(labor.reg),
                          frequency=period,names="LaborDay"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(col.reg),start=start(col.reg),
                          frequency=period,names="ColumbusDay"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(vet.reg),start=start(vet.reg),
                          frequency=period,names="VeteransDay"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(tg.reg),start=start(tg.reg),
                          frequency=period,names="Thanksgiving"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(xmas.reg),start=start(xmas.reg),
                          frequency=period,names="Xmas"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(black.reg),start=start(black.reg),
                          frequency=period,names="BlackFriday"))
``` 
 
- This specification has seven federal holidays, plus two other holidays and the 
trend constant, for a total of $10$ regressors.
- The model is fitted next.
  
```{r}  
constraint <- NULL
par.mle <- sigex.default(mdl,data.ts,constraint)
psi.mle <- sigex.par2psi(par.mle,mdl)
fit.mle <- sigex.mlefit(data.ts,par.mle,constraint,mdl,"bfgs",debug=FALSE)
psi.mle <- sigex.eta2psi(fit.mle[[1]]$par,constraint)
hess <- fit.mle[[1]]$hessian
par.mle <- fit.mle[[2]]
```

- This yields a divergence of `r sigex.lik(psi.mle,mdl,data.ts)`.
- We can also examine the Hessian matrix, and compute the t statistics for 
pre-parameters.

```{r}
print(eigen(hess)$values)
```

- We obtain, format, and store the residuals.
- Then we compute the autocorrelations, and plot.

```{r}
resid.mle <- sigex.resid(psi.mle,mdl,data.ts)[[1]]
resid.mle <- sigex.load(t(resid.mle),start(data.ts),frequency(data.ts),
                        colnames(data.ts),TRUE)
resid.acf <- acf(resid.mle,lag.max=4*53,plot=TRUE)$acf
```

- We observe in the fitted residuals that there is a substantial outlier in the 
first week of 2012, which is not explained by the New Year's regressor. 
- In a refined model below, this will be handled as an Additive Outlier.

### Final Model

- Now we retain the NewYears, MLK, and Labor Day holidays.
- Also we add an Additive Outlier (AO) effect at time point 314.
  
```{r}
AO.times <- 314
dataNA.ts <- data.ts
dataNA.ts[AO.times] <- NA
```

- Next, we construct the model.

```{r}
mdl <- NULL
mdl <- sigex.add(mdl,seq(1,N),"sarma",c(1,1,1,1,52),NULL,"process",1)
mdl <- sigex.meaninit(mdl,dataNA.ts,0)
mdl <- sigex.reg(mdl,1,ts(as.matrix(nyd.reg),start=start(nyd.reg),
                          frequency=period,names="NewYearDay"))
mdl <- sigex.reg(mdl,1,ts(as.matrix(mlk.reg),start=start(mlk.reg),
                          frequency=period,names="MLK"))
```

- The model is fitted next.
  
```{r}  
constraint <- NULL
par.mle <- sigex.default(mdl,dataNA.ts,constraint)
psi.mle <- sigex.par2psi(par.mle,mdl)
fit.mle <- sigex.mlefit(dataNA.ts,par.mle,constraint,mdl,"bfgs",debug=FALSE)
psi.mle <- sigex.eta2psi(fit.mle[[1]]$par,constraint)
hess <- fit.mle[[1]]$hessian
par.mle <- fit.mle[[2]]
```

- This yields a divergence of `r sigex.lik(psi.mle,mdl,data.ts)`.
- We can examine t statistics for the parameters.
 
```{r}
sigex.tstats(mdl,psi.mle,hess,constraint)
```

- We obtain, format, and store the residuals.
- Then we compute the autocorrelations, and plot.

```{r}
resid.mle <- sigex.resid(psi.mle,mdl,dataNA.ts)[[1]]
resid.mle <- sigex.load(t(resid.mle),start(data.ts),
                        frequency(data.ts),colnames(data.ts),TRUE)
resid.acf <- acf(resid.mle,lag.max=4*53,plot=FALSE)$acf
par(mfrow=c(N,N),mar=c(3,2,2,0)+0.1,cex.lab=.8,cex.axis=.5,bty="n")
for(j in 1:N)
{
  for(k in 1:N)
  {
    plot.ts(resid.acf[,j,k],ylab="",xlab="Lag",ylim=c(-1,1),cex=.5)
    abline(h=1.96/sqrt(T),lty=3)
    abline(h=-1.96/sqrt(T),lty=3)
  }
}
```

- Residual analysis indicates the model seems to be a good fit.
- We can examine the Portmanteau statistic for residual serial correlation. We 
use $48$ lags.
- We can also inspect normality via the  Shapiro-Wilks normality test.

```{r}
sigex.portmanteau(resid.mle,4*period,length(psi.mle))
sigex.gausscheck(resid.mle)
```

- We obtain the midcast (imputation for a missing value) for time $t=314$.
 
```{r}
data.casts <- sigex.midcast(psi.mle,mdl,dataNA.ts,0)
```

- Having completed our model analysis, we store our results in a single list 
object.

```{r}
analysis.mle <- sigex.bundle(dataNA.ts,transform,mdl,psi.mle)
```
 
## Signal Extraction

- The final stage of analysis is to filter the data.
- First we load up the bundled data.

```{r}
dataNA.ts <- analysis.mle[[1]]
mdl <- analysis.mle[[3]]
psi <- analysis.mle[[4]]
param <- sigex.psi2par(psi,mdl,dataNA.ts)
```

- The relevant fixed effects are loaded into variables.
- To do this we create a variable *dataLIN.ts*, consisting of the original time 
series with the fixed effects removed.
- This de-meaned series shall be adjusted for the AO, and then filtered.

```{r}
reg.trend <- sigex.fixed(data.ts,mdl,1,param,"Trend")
reg.nyd <- sigex.fixed(data.ts,mdl,1,param,"NewYearDay")
reg.mlk <- sigex.fixed(data.ts,mdl,1,param,"MLK")
dataLIN.ts <- data.ts - ts(reg.trend + reg.nyd + reg.mlk,
                           start=start(data.ts),frequency=period)
```

- We utilize a set of nonparametric filters to extract trend, seasonal, and 
non-seasonal (or seasonally adjusted) latent processes, implemented in  
**x11filters**.     
- Below, we define *week.period* with a non-integer period, which **x11filters**
is designed to accommodate.
- The variable *x11.filters* is a three-item list containing the three filters,
each of which is a symmetric sequence of numbers.   

```{r}
week.period <- 365.25/7
half.len <- floor(week.period/2)
x11.filters <- x11filters(week.period,1)
trend.filter <- x11.filters[[1]]
seas.filter <- x11.filters[[2]]
sa.filter <- x11.filters[[3]]
shift <- (dim(sa.filter)[3]-1)/2
```

- Next, we call **sigex.adhocextract** to apply the various ad hoc (AC) filters
to the data.
- We apply the method to *dataNA.ts*, which was defined previously as the 
de-meaned series with an NA inserted for the AO. 
- The AC filter *trend.filter* is inputted, along with the *shift* parameter set 
to the middle position of the filter vector; this corresponds to a symmetric 
filter.  
- We also compute the casting error at the AO time $t = 314$, and this is added 
onto the seasonal adjustment *sa.comp[[1]]*. This is done because an AO effect
belongs with the irregular in the seasonal adjustment extraction.
 
```{r}
trend.comp <- sigex.adhocextract(psi,mdl,dataNA.ts,trend.filter,half.len,0,TRUE)
sa.comp <- sigex.adhocextract(psi,mdl,dataNA.ts,sa.filter,shift,0,TRUE)
AO.errs <- dataLIN.ts[AO.times] - data.casts[[1]]
sa.comp[[1]][AO.times] <- sa.comp[[1]][AO.times] + AO.errs
```

- The results can be displayed using calls to **sigex.graph**.
- This code sets up various colors for the extractions, and a shading 
percentage *fade*.

```{r}
trendcol <- "tomato"
cyccol <- "orchid"
seascol <- "seagreen"
sacol <- "navyblue"
fade <- 60

plot(data.ts)
sigex.graph(trend.comp,reg.trend,start(data.ts),
            period,1,0,trendcol,fade)
sigex.graph(sa.comp,reg.trend,start(data.ts),
            period,1,0,sacol,fade)
```

- Next, we can examine spectral density estimates of the extractions.

```{r}
sigex.specar(sa.comp[[1]],FALSE,1,period)
```

- The lack of seasonal peaks indicates adequacy of the seasonal adjustment. 
 
  
